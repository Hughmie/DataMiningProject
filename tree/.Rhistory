data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 将目标变量转换为0/1
data$y <- ifelse(data$y == "yes", 1, 0)
# 3. 处理分类变量（不使用Matrix包）
# 方法1：标签编码（Label Encoding）
# 将因子变量转换为数值
data <- data %>%
mutate(across(where(is.factor), as.numeric))
# 4. 分割数据集
set.seed(0)  # 确保可重复性
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 5. 准备XGBoost输入数据
# 分离特征和目标变量
train_x <- as.matrix(train_data %>% select(-y))
train_y <- train_data$y
test_x <- as.matrix(test_data %>% select(-y))
test_y <- test_data$y
# 转换为XGBoost的DMatrix格式
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)
# 6. 设置XGBoost参数
params <- list(
objective = "binary:logistic",  # 二分类问题
eval_metric = "logloss",        # 评估指标
max_depth = 5,                  # 树的最大深度
eta = 0.3,                      # 学习率
subsample = 0.8,                # 样本采样比例
colsample_bytree = 0.8,         # 特征采样比例
min_child_weight = 1            # 叶节点最小样本权重
)
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 在训练集上评估模型
train_pred <- predict(xgb_model, train_data, type = "class")
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 200,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 150,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 6. 设置XGBoost参数
params <- list(
objective = "binary:logistic",  # 二分类问题
eval_metric = "logloss",        # 评估指标
max_depth = 5,                  # 树的最大深度
eta = 0.3,                      # 学习率
subsample = 0.8,                # 样本采样比例
colsample_bytree = 0.8,         # 特征采样比例
min_child_weight = 1            # 叶节点最小样本权重
)
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 150,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 评估模型（使用confusionMatrix）
test_pred_prob <- predict(xgb_model, dtest)
test_pred <- factor(ifelse(test_pred_prob > 0.5, "yes", "no"),
levels = c("no", "yes"))
test_actual <- factor(ifelse(test_data$y == 1, "yes", "no"),
levels = c("no", "yes"))
result <- confusionMatrix(test_pred, test_actual, positive = "yes")
print(result)
confusionMatrix(test_pred, test_actual, positive = "yes")
# ROC曲线
roc_obj <- roc(test_actual, test_pred_prob)
plot(roc_obj, print.auc = TRUE)
library(dplyr)
library(ggplot2)
# load data
data <- read.csv("datasets\\data_outliers_remain.csv")
# load data
data <- read.csv("../datasets\\data_outliers_remain.csv")
# find outliers
num_cols <- sapply(data, is.numeric) # find out which columns are numeric
find_outliers <- function(x) {
Q1 <- quantile(x, 0.25)
Q3 <- quantile(x, 0.75)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
sum(x < lower | x > upper) # count the number of outliers in each column
}
sapply(data[, num_cols], find_outliers)
summary(data)
# boxplot of 'campaign'
ggplot(data, aes(y = campaign)) +
geom_boxplot(outlier.colour = "red") +
labs(title = "Distribution of Campaign",
y = "Number of Contacts",
x = "")
# histogram of 'campaign'
ggplot(data, aes(x = campaign)) +
geom_histogram() +
labs(title = "Histogram of Campaign",
x = "Number of Contacts",
y = "")
# Truncate severe outliers from 'campaign' using Winsorizing
Q3 <- quantile(data$campaign, 0.75)
Q1 <- quantile(data$campaign, 0.25)
IQR <- Q3 - Q1
severe_upper <- Q3 + 3 * IQR
print(severe_upper)
data$campaign[data$campaign > severe_upper] <- severe_upper
summary(data$campaign)
# save data
write.csv(data, "datasets\\data_outliers_processed.csv", row.names = FALSE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
for(col in names(data)){
cat("\nColumn:", col, "\n")
print(table(data[[col]]))
}
library(neuralnet)
library(dplyr)
library(NeuralNetTools)
library(dplyr)
library(NeuralNetTools)
#import data
data <- read.csv("../datasets/data_outliers_processed.csv", stringsAsFactors = FALSE)
# inspect data
glimpse(data)
# categorical -> factor (except y)
# prepare for the conversion to dummy
cate_vars <- c("job","marital","education","default",
"housing","loan","contact","month",
"day_of_week","poutcome")
data[cate_vars] <- lapply(data[cate_vars], factor)
str(data)
# training-test random splitting (0.75:0.25)
set.seed(111)
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train_data <- data[index, ]
test_data <- data[-index, ]
# factor -> dummy
train_dummy <- model.matrix(~ job + marital + education + default +
housing + loan + contact + month +
day_of_week + poutcome - 1, data = train_data)
test_dummy <- model.matrix(~ job + marital + education + default +
housing + loan + contact + month +
day_of_week + poutcome - 1, data = test_data)
# min-max normalization
num_vars <- c("age","campaign","pdays","previous",
"emp.var.rate","cons.price.idx","cons.conf.idx",
"euribor3m","nr.employed")
train_num <- train_data[, num_vars]
test_num <- test_data[, num_vars]
maxs <- apply(train_num, 2, max)
mins <- apply(train_num, 2, min)
train_num_scaled <- scale(train_num, center = mins, scale = maxs - mins)
test_num_scaled  <- scale(test_num,  center = mins, scale = maxs - mins)
# merge the processed data: dummy, num_scaled and y
train_processed <- data.frame(as.data.frame(train_dummy),
as.data.frame(train_num_scaled),
y = as.numeric(ifelse(train_data$y == 'yes', 1, 0)))
test_processed <- data.frame(as.data.frame(test_dummy),
as.data.frame(test_num_scaled),
y = as.numeric(ifelse(test_data$y == 'yes', 1, 0)))
# inspect the distribution of y in train_processed
table(train_processed$y) # there is an imbalance between 'yes' and 'no'
# as the imbalance in outputs, use down-sampling
# separate the training samples into positive and negative
positive_samples <- train_processed[train_processed$y == 1, ]
negative_samples <- train_processed[train_processed$y == 0, ]
# set the ratio of sampling
n_positive_sampling <- nrow(positive_samples)
sampling_ratio <- 3
n_negative_sampling <- n_positive_sampling * sampling_ratio
# randomly select negative samples in training data
set.seed(222)
negative_indices_to_keep <- sample(1:nrow(negative_samples), n_negative_sampling)
negative_samples_downsampled <- negative_samples[negative_indices_to_keep, ]
# merge the positive/negative samples after down-sampling
train_balanced <- rbind(positive_samples, negative_samples_downsampled)
# ramdonly order the data
train_balanced <- train_balanced[sample(1:nrow(train_balanced)), ]
# inspect the distribution of y in training set again
table(train_balanced$y)
# build and train neural network
set.seed(333)
nn_model <- neuralnet(
y ~ .,
data = train_balanced,
hidden = c(20, 10, 5),
act.fct = "logistic",
linear.output = FALSE,
err.fct = "ce"
)
# test on test_processed
test_set <- subset(test_processed, select = -y) # truncate output and only use inputs
pred_result <- compute(nn_model, test_set) # forward propagation
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
# 检查缺失值
col_na <- colSums(is.na(data))
print("各列缺失值数量:")
print(colSums(is.na(data)))
# 四分位异常值
outlier_iqr <- function(x) {
Q <- quantile(x, probs=c(0.25, 0.75), na.rm=TRUE)
iqr <- IQR(x, na.rm=TRUE)
lower <- Q[1] - 1.5*iqr
upper <- Q[2] + 1.5*iqr
x < lower | x > upper
}
outliers <- outlier_iqr(data$campaign)
max(data$campaign)
# 检查数据结构
str(data)
### 数据处理
# 将y列转换为因子（如果是分类问题）
data$y <- as.factor(data$y)
for(col in names(data)){
cat("\nColumn:", col, "\n")
print(table(data[[col]]))
}
# 检查类别分布
table(data$y)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(balanced_train_data$y)
# 构建决策树模型
tree_model <- rpart(y ~ .,
data = balanced_train_data,
method = "class", # 用于分类
parms = list(split = "information"), # 使用信息增益
control = rpart.control(minsplit = 20, # 节点最小样本数
minbucket = 200,  # 叶节点最小样本数
maxdepth = 5,   # 树的最大深度
cp = 0.001))     # 复杂度参数
# 加载包
library(rpart)       # 用于构建决策树
library(rpart.plot)  # 用于可视化决策树
library(caret)       # 用于模型评估和数据处理
library(dplyr)
# 加载包
library(rpart)       # 用于构建决策树
library(rpart.plot)  # 用于可视化决策树
library(caret)       # 用于模型评估和数据处理
library(dplyr)
library(ROSE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
# 检查缺失值
col_na <- colSums(is.na(data))
print("各列缺失值数量:")
print(colSums(is.na(data)))
# 四分位异常值
outlier_iqr <- function(x) {
Q <- quantile(x, probs=c(0.25, 0.75), na.rm=TRUE)
iqr <- IQR(x, na.rm=TRUE)
lower <- Q[1] - 1.5*iqr
upper <- Q[2] + 1.5*iqr
x < lower | x > upper
}
outliers <- outlier_iqr(data$campaign)
max(data$campaign)
# 检查数据结构
str(data)
### 数据处理
# 将y列转换为因子（如果是分类问题）
data$y <- as.factor(data$y)
for(col in names(data)){
cat("\nColumn:", col, "\n")
print(table(data[[col]]))
}
# 检查类别分布
table(data$y)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(balanced_train_data$y)
# 构建决策树模型
tree_model <- rpart(y ~ .,
data = balanced_train_data,
method = "class", # 用于分类
parms = list(split = "information"), # 使用信息增益
control = rpart.control(minsplit = 20, # 节点最小样本数
minbucket = 200,  # 叶节点最小样本数
maxdepth = 5,   # 树的最大深度
cp = 0.001))     # 复杂度参数
test_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(test_pred, test_data$y,positive = "yes")
View(data)
library(randomForest)  # 随机森林算法
library(randomForest)  # 随机森林算法
library(caret)        # 数据分割和模型评估
library(ggplot2)      # 数据可视化
library(dplyr)        # 数据操作
library(ROCR)         # ROC曲线绘制
library(ROCR)         # ROC曲线绘制
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
data <- data %>% select(-duration)
#data <- data %>% select(-pdays)
data$pdays[data$pdays == 999] <- NA
# 查看数据结构
str(data)
# 将目标变量转换为因子
data$y <- as.factor(data$y)
# 检查类别分布（处理不平衡数据）
table(data$y)
prop.table(table(data$y))
set.seed(0)
# 创建训练集和测试集
train_Index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_Data <- data[trainIndex, ]
test_Data <- data[-trainIndex, ]
View(data)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(balanced_train_data$y)
# 构建决策树模型
tree_model <- rpart(y ~ .,
data = balanced_train_data,
method = "class", # 用于分类
parms = list(split = "information"), # 使用信息增益
control = rpart.control(minsplit = 20, # 节点最小样本数
minbucket = 200,  # 叶节点最小样本数
maxdepth = 5,   # 树的最大深度
cp = 0.001))     # 复杂度参数
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 加载包
library(rpart)       # 用于构建决策树
library(rpart.plot)  # 用于可视化决策树
library(caret)       # 用于模型评估和数据处理
library(dplyr)
library(ROSE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
#data <- data %>% select(-pdays)
data$pdays[data$pdays == 999] <- NA
# 检查缺失值
col_na <- colSums(is.na(data))
print("各列缺失值数量:")
print(colSums(is.na(data)))
# 四分位异常值
outlier_iqr <- function(x) {
Q <- quantile(x, probs=c(0.25, 0.75), na.rm=TRUE)
iqr <- IQR(x, na.rm=TRUE)
lower <- Q[1] - 1.5*iqr
upper <- Q[2] + 1.5*iqr
x < lower | x > upper
}
outliers <- outlier_iqr(data$campaign)
max(data$campaign)
# 检查数据结构
str(data)
### 数据处理
# 将y列转换为因子（如果是分类问题）
data$y <- as.factor(data$y)
for(col in names(data)){
cat("\nColumn:", col, "\n")
print(table(data[[col]]))
}
# 检查类别分布
table(data$y)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(balanced_train_data$y)
# 加载包
library(rpart)       # 用于构建决策树
library(rpart.plot)  # 用于可视化决策树
library(caret)       # 用于模型评估和数据处理
library(dplyr)
library(ROSE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
#data <- data %>% select(-pdays)
data$pdays[data$pdays == 999] <- NA
# 检查数据结构
str(data)
### 数据处理
# 将y列转换为因子（如果是分类问题）
data$y <- as.factor(data$y)
# 检查类别分布
table(data$y)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
table(train_data$y)
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(train_data$y)
table(balanced_train_data$y)
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "both", N = 12410)$data
table(train_data$y)
table(balanced_train_data$y)
# 构建决策树模型
tree_model <- rpart(y ~ .,
data = balanced_train_data,
method = "class", # 用于分类
parms = list(split = "information"), # 使用信息增益
control = rpart.control(minsplit = 20, # 节点最小样本数
minbucket = 200,  # 叶节点最小样本数
maxdepth = 5,   # 树的最大深度
cp = 0.001))     # 复杂度参数
test_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(test_pred, test_data$y,positive = "yes")
# 查看模型摘要
print(tree_model)
summary(tree_model)
# 可视化决策树
rpart.plot(tree_model,
type = 4,
extra = 104,
box.palette = "GnBu",
branch.lty = 3,
shadow.col = "gray",
nn = TRUE)
# 在训练集上评估模型
train_pred <- predict(tree_model, train_data, type = "class")
confusionMatrix(train_pred, train_data$y,positive = "yes")
