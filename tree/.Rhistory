# 循环测试不同参数组合
for (ms in minsplit_range) {
for (mb in minbucket_range) {
for (md in maxdepth_range) {
# 训练模型
model <- rpart(y ~ .,
data = balanced_train_data,
method = "class",
control = rpart.control(minsplit = ms,
minbucket = mb,
maxdepth = md,
cp = 0.001))
# 预测并计算指标
pred <- predict(model, test_data, type = "class")
cm <- confusionMatrix(pred, test_data$y, positive = "yes")
# 提取指标
accuracy <- cm$overall['Accuracy']
sensitivity <- cm$byClass['Sensitivity']
precision <- cm$byClass['Pos Pred Value']
# 打印结果（保留4位小数）
cat(sprintf("minsplit=%3d, minbucket=%3d, maxdepth=%2d | ",
ms, mb, md))
cat(sprintf("Accuracy=%.4f, Sensitivity=%.4f, Precision=%.4f\n",
accuracy, sensitivity, precision))
}
}
}
# 加载包
library(rpart)       # 用于构建决策树
library(rpart.plot)  # 用于可视化决策树
library(caret)       # 用于模型评估和数据处理
library(dplyr)
library(ROSE)
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 检查缺失值
col_na <- colSums(is.na(data))
print("各列缺失值数量:")
print(colSums(is.na(data)))
# 四分位异常值
outlier_iqr <- function(x) {
Q <- quantile(x, probs=c(0.25, 0.75), na.rm=TRUE)
iqr <- IQR(x, na.rm=TRUE)
lower <- Q[1] - 1.5*iqr
upper <- Q[2] + 1.5*iqr
x < lower | x > upper
}
outliers <- outlier_iqr(data$campaign)
max(data$campaign)
# 检查数据结构
str(data)
### 数据处理
# 将y列转换为因子（如果是分类问题）
data$y <- as.factor(data$y)
# 检查类别分布
table(data$y)
# 分割数据为训练集和测试集
set.seed(0) # 确保结果可重现
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 超采样
balanced_train_data <- ovun.sample(y ~ ., data = train_data, method = "over", N = 12410)$data
table(balanced_train_data$y)
# 构建决策树模型
tree_model <- rpart(y ~ .,
data = balanced_train_data,
method = "class", # 用于分类
parms = list(split = "information"), # 使用信息增益
control = rpart.control(minsplit = 20, # 节点最小样本数
minbucket = 200,  # 叶节点最小样本数
maxdepth = 5,   # 树的最大深度
cp = 0.001))     # 复杂度参数
test_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(test_pred, test_data$y,positive = "yes")
# 查看模型摘要
print(tree_model)
summary(tree_model)
# 可视化决策树
rpart.plot(tree_model,
type = 4,
extra = 104,
box.palette = "GnBu",
branch.lty = 3,
shadow.col = "gray",
nn = TRUE)
# 在训练集上评估模型
train_pred <- predict(tree_model, train_data, type = "class")
confusionMatrix(train_pred, train_data$y,positive = "yes")
# 在测试集上评估模型
test_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(test_pred, test_data$y,positive = "yes")
test_pred_prob <- predict(tree_model, test_data, type = "prob")[, "yes"]
roc_obj <- roc(test_data$y, test_pred_prob, levels = c("no", "yes"))
# 绘制ROC曲线
plot(roc_obj,
main = "ROC Curve for Decision Tree Model",
col = "blue",
lwd = 2,
print.auc = TRUE,
auc.polygon = TRUE,
auc.polygon.col = "lightblue",
print.thres = TRUE)
# 计算AUC值
auc_value <- auc(roc_obj)
cat("AUC值:", auc_value, "\n")
# 查看变量重要性
tree_model$variable.importance
# 绘制CP表格以选择最佳剪枝
plotcp(tree_model)
test_pred <- predict(tree_model, test_data, type = "prob")[, "yes"]
roc_obj <- roc(test_data$y, test_pred, levels = c("no", "yes"))
# 绘制ROC曲线
plot(roc_obj,
main = "ROC Curve for Decision Tree Model",
col = "blue",
lwd = 2,
print.auc = TRUE,
auc.polygon = TRUE,
auc.polygon.col = "lightblue",
print.thres = TRUE)
# 计算AUC值
auc_value <- auc(roc_obj)
cat("AUC值:", auc_value, "\n")
library(randomForest)  # 随机森林算法
library(caret)        # 数据分割和模型评估
library(ggplot2)      # 数据可视化
library(dplyr)        # 数据操作
library(ROCR)         # ROC曲线绘制
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 查看数据结构
str(data)
# 将目标变量转换为因子
data$y <- as.factor(data$y)
# 检查类别分布（处理不平衡数据）
table(data$y)
prop.table(table(data$y))
set.seed(123)
# 创建训练集和测试集
trainIndex <- createDataPartition(data$y, p = 0.7, list = FALSE)
library(randomForest)  # 随机森林算法
library(caret)        # 数据分割和模型评估
library(ggplot2)      # 数据可视化
library(dplyr)        # 数据操作
library(ROCR)         # ROC曲线绘制
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 查看数据结构
str(data)
# 将目标变量转换为因子
data$y <- as.factor(data$y)
# 检查类别分布（处理不平衡数据）
table(data$y)
prop.table(table(data$y))
set.seed(123)
library(randomForest)  # 随机森林算法
library(caret)        # 数据分割和模型评估
library(ggplot2)      # 数据可视化
library(dplyr)        # 数据操作
library(ROCR)         # ROC曲线绘制
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 查看数据结构
str(data)
# 将目标变量转换为因子
data$y <- as.factor(data$y)
# 检查类别分布（处理不平衡数据）
table(data$y)
prop.table(table(data$y))
set.seed(0)
# 创建训练集和测试集
train_Index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_Data <- data[trainIndex, ]
test_Data <- data[-trainIndex, ]
# 检查分割后的类别分布
table(train_Data$y)
table(test_Data$y)
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 100,          # 树的数量
mtry = sqrt(ncol(trainData) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 100,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, testData)
library(randomForest)  # 随机森林算法
library(caret)        # 数据分割和模型评估
library(ggplot2)      # 数据可视化
library(dplyr)        # 数据操作
library(ROCR)         # ROC曲线绘制
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 查看数据结构
str(data)
# 将目标变量转换为因子
data$y <- as.factor(data$y)
# 检查类别分布（处理不平衡数据）
table(data$y)
prop.table(table(data$y))
set.seed(0)
# 创建训练集和测试集
train_Index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_Data <- data[trainIndex, ]
test_Data <- data[-trainIndex, ]
# 检查分割后的类别分布
table(train_Data$y)
table(test_Data$y)
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 100,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, testData)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 200,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
# 计算预测概率（用于ROC曲线）
predict_prob <- predict(rf_model, test_Data, type = "prob")[, "yes"]
# 创建预测对象
pred <- prediction(predict_prob, test_Data$y)
# 计算性能指标
perf <- performance(pred, "tpr", "fpr")
# 绘制ROC曲线
plot(perf, colorize = TRUE, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2)
# 计算AUC值
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 88,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
# 计算预测概率（用于ROC曲线）
predict_prob <- predict(rf_model, test_Data, type = "prob")[, "yes"]
# 创建预测对象
pred <- prediction(predict_prob, test_Data$y)
# 计算性能指标
perf <- performance(pred, "tpr", "fpr")
# 绘制ROC曲线
plot(perf, colorize = TRUE, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2)
# 计算AUC值
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 150,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
# 计算预测概率（用于ROC曲线）
predict_prob <- predict(rf_model, test_Data, type = "prob")[, "yes"]
# 创建预测对象
pred <- prediction(predict_prob, test_Data$y)
# 计算性能指标
perf <- performance(pred, "tpr", "fpr")
# 绘制ROC曲线
plot(perf, colorize = TRUE, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2)
# 计算AUC值
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 166,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
# 计算预测概率（用于ROC曲线）
predict_prob <- predict(rf_model, test_Data, type = "prob")[, "yes"]
# 创建预测对象
pred <- prediction(predict_prob, test_Data$y)
# 计算性能指标
perf <- performance(pred, "tpr", "fpr")
# 绘制ROC曲线
plot(perf, colorize = TRUE, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2)
# 计算AUC值
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
rf_model <- randomForest(y ~ .,
data = train_Data,
ntree = 149,          # 树的数量
mtry = sqrt(ncol(train_Data) - 1),  # 每棵树使用的特征数
importance = TRUE,    # 计算特征重要性
proximity = TRUE,     # 计算样本相似度
strata = train_Data$y, # 用于分层抽样
sampsize = c("no" = sum(train_Data$y == "no"),
"yes" = sum(train_Data$y == "yes"))) # 平衡样本大小
# 查看模型摘要
print(rf_model)
predictions <- predict(rf_model, test_Data)
# 混淆矩阵
confusionMatrix(predictions, test_Data$y,positive = "yes")
# 计算预测概率（用于ROC曲线）
predict_prob <- predict(rf_model, test_Data, type = "prob")[, "yes"]
# 创建预测对象
pred <- prediction(predict_prob, test_Data$y)
# 计算性能指标
perf <- performance(pred, "tpr", "fpr")
# 绘制ROC曲线
plot(perf, colorize = TRUE, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2)
# 计算AUC值
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC:", auc, "\n")
library(xgboost)    # XGBoost模型
library(caret)      # 数据分割和预处理
library(dplyr)      # 数据操作
library(pROC)       # ROC曲线评估
# 2. 数据准备和预处理
# 读取数据
data <- read.csv("group_23.csv", stringsAsFactors = TRUE)
#删除 duration列
data <- data %>% select(-duration)
data <- data %>% select(-pdays)
# 将目标变量转换为0/1
data$y <- ifelse(data$y == "yes", 1, 0)
# 3. 处理分类变量（不使用Matrix包）
# 方法1：标签编码（Label Encoding）
# 将因子变量转换为数值
data <- data %>%
mutate(across(where(is.factor), as.numeric))
# 4. 分割数据集
set.seed(0)  # 确保可重复性
train_index <- createDataPartition(data$y, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 5. 准备XGBoost输入数据
# 分离特征和目标变量
train_x <- as.matrix(train_data %>% select(-y))
train_y <- train_data$y
test_x <- as.matrix(test_data %>% select(-y))
test_y <- test_data$y
# 转换为XGBoost的DMatrix格式
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)
# 6. 设置XGBoost参数
params <- list(
objective = "binary:logistic",  # 二分类问题
eval_metric = "logloss",        # 评估指标
max_depth = 5,                  # 树的最大深度
eta = 0.3,                      # 学习率
subsample = 0.8,                # 样本采样比例
colsample_bytree = 0.8,         # 特征采样比例
min_child_weight = 1            # 叶节点最小样本权重
)
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 在训练集上评估模型
train_pred <- predict(xgb_model, train_data, type = "class")
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 200,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 150,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 8. 模型评估
# 预测概率
pred_prob <- predict(xgb_model, dtest)
# 转换为类别预测
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
# 混淆矩阵
conf_matrix <- table(Predicted = pred_class, Actual = test_y)
print("混淆矩阵:")
print(conf_matrix)
# 计算准确率
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("准确率:", round(accuracy, 4)))
# 6. 设置XGBoost参数
params <- list(
objective = "binary:logistic",  # 二分类问题
eval_metric = "logloss",        # 评估指标
max_depth = 5,                  # 树的最大深度
eta = 0.3,                      # 学习率
subsample = 0.8,                # 样本采样比例
colsample_bytree = 0.8,         # 特征采样比例
min_child_weight = 1            # 叶节点最小样本权重
)
# 7. 训练模型
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 150,                  # 迭代次数
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,     # 早停轮数
print_every_n = 10              # 打印频率
)
# 评估模型（使用confusionMatrix）
test_pred_prob <- predict(xgb_model, dtest)
test_pred <- factor(ifelse(test_pred_prob > 0.5, "yes", "no"),
levels = c("no", "yes"))
test_actual <- factor(ifelse(test_data$y == 1, "yes", "no"),
levels = c("no", "yes"))
result <- confusionMatrix(test_pred, test_actual, positive = "yes")
print(result)
confusionMatrix(test_pred, test_actual, positive = "yes")
# ROC曲线
roc_obj <- roc(test_actual, test_pred_prob)
plot(roc_obj, print.auc = TRUE)
